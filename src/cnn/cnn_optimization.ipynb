{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2fea45111280c3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T02:43:48.685297Z",
     "start_time": "2024-07-18T02:43:43.570690Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\") # Add parent directory to sys.path to access preprocessing module\n",
    "\n",
    "from preprocessing.feature_extractor import FeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T02:43:57.989888Z",
     "start_time": "2024-07-18T02:43:55.889750Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing features from ../../data/Composer_Dataset/extracted_features.pkl\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"../../data/Composer_Dataset\"\n",
    "composers = [\"Bach\", \"Beethoven\", \"Chopin\", \"Mozart\"]\n",
    "scalar_features, multidimensional_features = FeatureExtractor.extract_features_for_directory(data_dir, composers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e5db583236dfbcb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T02:44:04.615360Z",
     "start_time": "2024-07-18T02:44:04.597740Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_independent_voices</th>\n",
       "      <th>avg_independent_voices</th>\n",
       "      <th>var_independent_voices</th>\n",
       "      <th>avg_simultaneity</th>\n",
       "      <th>var_simultaneity</th>\n",
       "      <th>note_density</th>\n",
       "      <th>avg_note_duration</th>\n",
       "      <th>var_note_duration</th>\n",
       "      <th>initial_tempo</th>\n",
       "      <th>time_signature_numerator</th>\n",
       "      <th>...</th>\n",
       "      <th>perfect_vertical_intervals</th>\n",
       "      <th>vertical_minor_seconds</th>\n",
       "      <th>vertical_thirds</th>\n",
       "      <th>vertical_fifths</th>\n",
       "      <th>vertical_tritones</th>\n",
       "      <th>vertical_octaves</th>\n",
       "      <th>avg_chord_duration</th>\n",
       "      <th>length</th>\n",
       "      <th>file_name</th>\n",
       "      <th>composer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.899642</td>\n",
       "      <td>0.300477</td>\n",
       "      <td>3.720000</td>\n",
       "      <td>1.020588</td>\n",
       "      <td>6.439026</td>\n",
       "      <td>0.621212</td>\n",
       "      <td>0.296121</td>\n",
       "      <td>143.000038</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.391195</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.327044</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.045283</td>\n",
       "      <td>0.122013</td>\n",
       "      <td>0.465000</td>\n",
       "      <td>29.999992</td>\n",
       "      <td>042100b_.mid</td>\n",
       "      <td>Bach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.986667</td>\n",
       "      <td>0.114698</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>7.394595</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.250579</td>\n",
       "      <td>189.176471</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.400447</td>\n",
       "      <td>0.005220</td>\n",
       "      <td>0.307979</td>\n",
       "      <td>0.168531</td>\n",
       "      <td>0.042506</td>\n",
       "      <td>0.123788</td>\n",
       "      <td>0.321429</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>043100b_.mid</td>\n",
       "      <td>Bach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2.664012</td>\n",
       "      <td>1.617631</td>\n",
       "      <td>2.331998</td>\n",
       "      <td>1.816512</td>\n",
       "      <td>6.292553</td>\n",
       "      <td>0.371665</td>\n",
       "      <td>0.686803</td>\n",
       "      <td>181.905446</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.378734</td>\n",
       "      <td>0.006634</td>\n",
       "      <td>0.291590</td>\n",
       "      <td>0.120602</td>\n",
       "      <td>0.057594</td>\n",
       "      <td>0.158185</td>\n",
       "      <td>0.177695</td>\n",
       "      <td>923.701235</td>\n",
       "      <td>Bwv0564-Toccata-Adagio-and-Fugue.mid</td>\n",
       "      <td>Bach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.970874</td>\n",
       "      <td>0.168160</td>\n",
       "      <td>3.662222</td>\n",
       "      <td>1.112213</td>\n",
       "      <td>8.775758</td>\n",
       "      <td>0.455801</td>\n",
       "      <td>0.212811</td>\n",
       "      <td>177.509434</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.375205</td>\n",
       "      <td>0.009852</td>\n",
       "      <td>0.312808</td>\n",
       "      <td>0.130542</td>\n",
       "      <td>0.030378</td>\n",
       "      <td>0.115764</td>\n",
       "      <td>0.321875</td>\n",
       "      <td>22.500000</td>\n",
       "      <td>027400b_.mid</td>\n",
       "      <td>Bach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.917323</td>\n",
       "      <td>0.275394</td>\n",
       "      <td>3.455782</td>\n",
       "      <td>1.371385</td>\n",
       "      <td>5.195127</td>\n",
       "      <td>0.713614</td>\n",
       "      <td>0.442007</td>\n",
       "      <td>153.000153</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.478439</td>\n",
       "      <td>0.002738</td>\n",
       "      <td>0.309377</td>\n",
       "      <td>0.206023</td>\n",
       "      <td>0.016427</td>\n",
       "      <td>0.140999</td>\n",
       "      <td>0.470370</td>\n",
       "      <td>29.333304</td>\n",
       "      <td>026400b_.mid</td>\n",
       "      <td>Bach</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   max_independent_voices  avg_independent_voices  var_independent_voices  \\\n",
       "0                     4.0                3.899642                0.300477   \n",
       "1                     4.0                3.986667                0.114698   \n",
       "2                     7.0                2.664012                1.617631   \n",
       "3                     4.0                3.970874                0.168160   \n",
       "4                     4.0                3.917323                0.275394   \n",
       "\n",
       "   avg_simultaneity  var_simultaneity  note_density  avg_note_duration  \\\n",
       "0          3.720000          1.020588      6.439026           0.621212   \n",
       "1          3.600000          1.200000      7.394595           0.526316   \n",
       "2          2.331998          1.816512      6.292553           0.371665   \n",
       "3          3.662222          1.112213      8.775758           0.455801   \n",
       "4          3.455782          1.371385      5.195127           0.713614   \n",
       "\n",
       "   var_note_duration  initial_tempo  time_signature_numerator  ...  \\\n",
       "0           0.296121     143.000038                       4.0  ...   \n",
       "1           0.250579     189.176471                       4.0  ...   \n",
       "2           0.686803     181.905446                       4.0  ...   \n",
       "3           0.212811     177.509434                       4.0  ...   \n",
       "4           0.442007     153.000153                       4.0  ...   \n",
       "\n",
       "   perfect_vertical_intervals  vertical_minor_seconds  vertical_thirds  \\\n",
       "0                    0.391195                0.000000         0.327044   \n",
       "1                    0.400447                0.005220         0.307979   \n",
       "2                    0.378734                0.006634         0.291590   \n",
       "3                    0.375205                0.009852         0.312808   \n",
       "4                    0.478439                0.002738         0.309377   \n",
       "\n",
       "   vertical_fifths  vertical_tritones  vertical_octaves  avg_chord_duration  \\\n",
       "0         0.200000           0.045283          0.122013            0.465000   \n",
       "1         0.168531           0.042506          0.123788            0.321429   \n",
       "2         0.120602           0.057594          0.158185            0.177695   \n",
       "3         0.130542           0.030378          0.115764            0.321875   \n",
       "4         0.206023           0.016427          0.140999            0.470370   \n",
       "\n",
       "       length                             file_name  composer  \n",
       "0   29.999992                          042100b_.mid      Bach  \n",
       "1   25.000000                          043100b_.mid      Bach  \n",
       "2  923.701235  Bwv0564-Toccata-Adagio-and-Fugue.mid      Bach  \n",
       "3   22.500000                          027400b_.mid      Bach  \n",
       "4   29.333304                          026400b_.mid      Bach  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalar_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b80c176ddbd29940",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T02:44:07.269877Z",
     "start_time": "2024-07-18T02:44:07.266095Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Piano Roll Shape: (128, 299)\n",
      "Chroma Piano Roll Shape: (12, 299)\n",
      "Pitch Class Histogram Shape: (12,)\n",
      "Pitch Class Transition Matrix Shape: (12, 12)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Piano Roll Shape: {multidimensional_features[0]['piano_roll'].shape}\")\n",
    "print(f\"Chroma Piano Roll Shape: {multidimensional_features[0]['chroma_piano_roll'].shape}\")\n",
    "print(f\"Pitch Class Histogram Shape: {multidimensional_features[0]['pitch_class_histogram'].shape}\")\n",
    "print(f\"Pitch Class Transition Matrix Shape: {multidimensional_features[0]['pitch_class_transition_matrix'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2f049f",
   "metadata": {},
   "source": [
    "## CNN Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0074e8",
   "metadata": {},
   "source": [
    "### Train Val Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bc869cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = 0.8  # ratio of all data to use for training\n",
    "val_test_split = 0.5  # ratio of holdout data to use for test set\n",
    "\n",
    "x = multidimensional_features\n",
    "y = scalar_features['composer']\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "x_train, x_holdout, y_train, y_holdout = train_test_split(x, y, test_size=(1 - train_split), random_state=1)\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_holdout, y_holdout, test_size=val_test_split, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec258f21",
   "metadata": {},
   "source": [
    "### Chunk Into Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c06b249a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_sequences(X_in, y_in, feature, sequence_length=100):\n",
    "    X_out = []\n",
    "    y_out = []\n",
    "\n",
    "    # make sequences split along the time axis\n",
    "    for i in range(len(X_in)):\n",
    "        for j in range(0, len(X_in[i][feature][1]) - sequence_length, sequence_length):\n",
    "            X_out.append(X_in[i][feature][:, j:j + sequence_length])\n",
    "            y_out.append(y_in[i])\n",
    "\n",
    "    return X_out, y_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfa3f0f",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ab6134",
   "metadata": {},
   "source": [
    "### Make Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcc5b271",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PianoRollDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = torch.tensor(self.X[idx], dtype=torch.float32) \n",
    "        y = torch.tensor(self.y[idx], dtype=torch.long)\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48485ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(use_chroma, seq_length=100, batch_size=8):\n",
    "\n",
    "\n",
    "    # Set random seed for PyTorch\n",
    "    torch.manual_seed(42) \n",
    "    \n",
    "    feature_set = 'chroma_piano_roll' if use_chroma else 'piano_roll'\n",
    "\n",
    "    X_train_seq, y_train_seq = chunk_sequences(x_train, y_train, feature_set, seq_length)\n",
    "    X_val_seq, y_val_seq = chunk_sequences(x_val, y_val, feature_set, seq_length)\n",
    "    X_test_seq, y_test_seq = chunk_sequences(x_test, y_test, feature_set, seq_length)\n",
    "\n",
    "    train_dataset = PianoRollDataset(X_train_seq, y_train_seq)\n",
    "    val_dataset = PianoRollDataset(X_val_seq, y_val_seq)\n",
    "    test_dataset = PianoRollDataset(X_test_seq, y_test_seq)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec00e3c2",
   "metadata": {},
   "source": [
    "### Define CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a022e8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PianoRollCNN(nn.Module):\n",
    "    def __init__(self, h_params):\n",
    "        super(PianoRollCNN, self).__init__()\n",
    "        # Conv1d layers: input channels = 12, height = 1, width = 50\n",
    "\n",
    "        input_channels = 12 if h_params['use_chroma'] else 128\n",
    "\n",
    "        self.conv1 = nn.Conv1d(input_channels, h_params['conv1_out_channels'], \n",
    "                               kernel_size=h_params['kernel_size'], stride=h_params['stride'], \n",
    "                               padding=h_params['kernel_size'] // 2)\n",
    "        self.conv2 = nn.Conv1d(h_params['conv1_out_channels'], h_params['conv2_out_channels'], \n",
    "                               kernel_size=h_params['kernel_size'], stride=h_params['stride'],\n",
    "                               padding=h_params['kernel_size'] // 2)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=h_params['pool_kernel_size'], stride=h_params['pool_stride'], padding=0)\n",
    "\n",
    "        def conv_output_size(input_size, kernel_size, stride, padding):\n",
    "            return (input_size - kernel_size + 2 * padding) // stride + 1\n",
    "    \n",
    "        padding = h_params['kernel_size'] // 2  # Assuming same padding\n",
    "\n",
    "        # First conv layer output size\n",
    "        conv1_output_width = conv_output_size(int(h_params['seq_length']), h_params['kernel_size'], h_params['stride'], padding)\n",
    "        # First pooling layer output size\n",
    "        pooled_width = conv_output_size(conv1_output_width, h_params['pool_kernel_size'], h_params['pool_stride'], 0)\n",
    "        # Second conv layer output size\n",
    "        conv2_output_width = conv_output_size(pooled_width, h_params['kernel_size'], h_params['stride'], padding)\n",
    "        # Second pooling layer output size\n",
    "        pooled_width = conv_output_size(conv2_output_width, h_params['pool_kernel_size'], h_params['pool_stride'], 0)\n",
    "\n",
    "        self.fc1 = nn.Linear(h_params['conv2_out_channels'] * pooled_width, h_params['fc1_out'])\n",
    "        self.dropout = nn.Dropout(h_params['dropout'])\n",
    "        self.fc2 = nn.Linear(h_params['fc1_out'], 4)  # Output of 4 classes (composers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.squeeze(2) # This can potentially be removed if the input is reshaped correctly in the dataloader\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1) \n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca495a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cnn(model, train_loader, val_loader, num_epochs, criterion, optimizer):\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        report_interval = 1000\n",
    "        for i, data in enumerate(train_loader):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            if i % report_interval == (report_interval - 1):\n",
    "                print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / report_interval:.3f}')\n",
    "                running_loss = 0.0  # Reset running loss\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data in val_loader:\n",
    "                inputs, labels = data\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        print(f'Validation loss: {val_loss / len(val_loader):.3f}, Accuracy: {100 * correct / total:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d097bcee",
   "metadata": {},
   "source": [
    "### Evaluate Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08982092",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_performance(model, data_loader):\n",
    "    model.eval()\n",
    "\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    # Disable gradient computation for evaluation\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            inputs, labels = data\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            # Store true and predicted labels\n",
    "            all_labels.extend(labels.numpy())\n",
    "            all_predictions.extend(predicted.numpy())\n",
    "\n",
    "    # Compute the classification report\n",
    "    report = classification_report(all_labels, all_predictions, target_names=label_encoder.classes_)\n",
    "    print(report)\n",
    "\n",
    "    return report, all_labels, all_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76afcd7e",
   "metadata": {},
   "source": [
    "### Implement One Training Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f235ed85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_run(h_params):\n",
    "\n",
    "    seq_length = int(h_params['seq_length'])\n",
    "    batch_size = int(h_params['batch_size'])\n",
    "\n",
    "    train_loader, val_loader, test_loader = get_dataloaders(h_params['use_chroma'], seq_length, batch_size)\n",
    "\n",
    "    model = PianoRollCNN(h_params)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=h_params['lr'])\n",
    "    train_cnn(model, train_loader, val_loader, h_params['num_epochs'], criterion, optimizer)\n",
    "\n",
    "    # TODO: Add model save \n",
    "    ###### Save results #####\n",
    "    return model, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52a3fdc",
   "metadata": {},
   "source": [
    "### Process All Hyperparameter Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eadeb994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process the file and train models\n",
    "def process_file(file_path):\n",
    "    infile = pd.read_csv(file_path)\n",
    "\n",
    "    outputs = ['accuracy', 'bach_f1', 'beethoven_f1', 'chopin_f1', 'mozart_f1']\n",
    "    floating_point_columns = ['lr']\n",
    "\n",
    "    for index, row in infile.iterrows():\n",
    "\n",
    "        # Check that the row has not been processed\n",
    "        if not pd.isnull(row['accuracy']):\n",
    "            continue\n",
    "\n",
    "        # Create hyperparameter dictionary dynamically\n",
    "        hyperparams = {col: row[col] for col in infile.columns if col not in outputs}\n",
    "        # Convert int columns to int\n",
    "        for col in infile.columns:\n",
    "            if col not in floating_point_columns and col not in outputs:\n",
    "                hyperparams[col] = int(hyperparams[col])\n",
    "            elif col in floating_point_columns:\n",
    "                hyperparams[col] = float(hyperparams[col])\n",
    "\n",
    "        # Train model\n",
    "        model, test_loader = training_run(hyperparams)\n",
    "\n",
    "        # Evaluate performance\n",
    "        report, all_labels, all_predictions = evaluate_performance(model, test_loader)\n",
    "\n",
    "        # Write accuracy to data then save\n",
    "        infile.at[index, 'accuracy'] = float(report.split('\\n')[7].split()[1])\n",
    "        infile.at[index, 'bach_f1'] = float(report.split('\\n')[2].split()[3])\n",
    "        infile.at[index, 'beethoven_f1'] = float(report.split('\\n')[3].split()[3])\n",
    "        infile.at[index, 'chopin_f1'] = float(report.split('\\n')[4].split()[3])\n",
    "        infile.at[index, 'mozart_f1'] = float(report.split('\\n')[5].split()[3])\n",
    "        infile.to_csv(file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a75daa2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1, Batch 1000] loss: 1.299\n",
      "[Epoch 1, Batch 2000] loss: 1.197\n",
      "[Epoch 1, Batch 3000] loss: 1.160\n",
      "[Epoch 1, Batch 4000] loss: 1.124\n",
      "[Epoch 1, Batch 5000] loss: 1.096\n",
      "[Epoch 1, Batch 6000] loss: 1.093\n",
      "[Epoch 1, Batch 7000] loss: 1.070\n",
      "[Epoch 1, Batch 8000] loss: 1.057\n",
      "Validation loss: 1.071, Accuracy: 51.58%\n",
      "[Epoch 2, Batch 1000] loss: 1.054\n",
      "[Epoch 2, Batch 2000] loss: 1.029\n",
      "[Epoch 2, Batch 3000] loss: 1.028\n",
      "[Epoch 2, Batch 4000] loss: 1.021\n",
      "[Epoch 2, Batch 5000] loss: 1.015\n",
      "[Epoch 2, Batch 6000] loss: 0.980\n",
      "[Epoch 2, Batch 7000] loss: 0.996\n",
      "[Epoch 2, Batch 8000] loss: 0.991\n",
      "Validation loss: 1.010, Accuracy: 54.33%\n",
      "[Epoch 3, Batch 1000] loss: 0.961\n",
      "[Epoch 3, Batch 2000] loss: 0.961\n",
      "[Epoch 3, Batch 3000] loss: 0.969\n",
      "[Epoch 3, Batch 4000] loss: 0.965\n",
      "[Epoch 3, Batch 5000] loss: 0.961\n",
      "[Epoch 3, Batch 6000] loss: 0.961\n",
      "[Epoch 3, Batch 7000] loss: 0.978\n",
      "[Epoch 3, Batch 8000] loss: 0.962\n",
      "Validation loss: 1.008, Accuracy: 54.66%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Bach       0.76      0.84      0.80      3290\n",
      "   Beethoven       0.47      0.40      0.43      1814\n",
      "      Chopin       0.91      0.28      0.42      1047\n",
      "      Mozart       0.25      0.42      0.31       968\n",
      "\n",
      "    accuracy                           0.59      7119\n",
      "   macro avg       0.60      0.48      0.49      7119\n",
      "weighted avg       0.64      0.59      0.58      7119\n",
      "\n",
      "[Epoch 1, Batch 1000] loss: 1.348\n",
      "[Epoch 1, Batch 2000] loss: 1.150\n",
      "[Epoch 1, Batch 3000] loss: 1.097\n",
      "Validation loss: 1.081, Accuracy: 48.47%\n",
      "[Epoch 2, Batch 1000] loss: 1.018\n",
      "[Epoch 2, Batch 2000] loss: 0.998\n",
      "[Epoch 2, Batch 3000] loss: 1.007\n",
      "Validation loss: 1.002, Accuracy: 57.12%\n",
      "[Epoch 3, Batch 1000] loss: 0.925\n",
      "[Epoch 3, Batch 2000] loss: 0.934\n",
      "[Epoch 3, Batch 3000] loss: 0.936\n",
      "Validation loss: 1.030, Accuracy: 54.78%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Bach       0.81      0.89      0.85      1619\n",
      "   Beethoven       0.51      0.58      0.54       901\n",
      "      Chopin       0.80      0.33      0.47       518\n",
      "      Mozart       0.30      0.30      0.30       481\n",
      "\n",
      "    accuracy                           0.65      3519\n",
      "   macro avg       0.60      0.53      0.54      3519\n",
      "weighted avg       0.66      0.65      0.64      3519\n",
      "\n",
      "[Epoch 1, Batch 1000] loss: 1.314\n",
      "Validation loss: 1.062, Accuracy: 53.44%\n",
      "[Epoch 2, Batch 1000] loss: 0.992\n",
      "Validation loss: 0.946, Accuracy: 58.44%\n",
      "[Epoch 3, Batch 1000] loss: 0.871\n",
      "Validation loss: 0.989, Accuracy: 55.71%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Bach       0.74      0.89      0.81       788\n",
      "   Beethoven       0.51      0.45      0.48       445\n",
      "      Chopin       0.86      0.37      0.52       254\n",
      "      Mozart       0.21      0.25      0.23       236\n",
      "\n",
      "    accuracy                           0.61      1723\n",
      "   macro avg       0.58      0.49      0.51      1723\n",
      "weighted avg       0.63      0.61      0.60      1723\n",
      "\n",
      "[Epoch 1, Batch 1000] loss: 1.340\n",
      "Validation loss: 1.148, Accuracy: 46.67%\n",
      "[Epoch 2, Batch 1000] loss: 1.058\n",
      "Validation loss: 0.997, Accuracy: 56.74%\n",
      "[Epoch 3, Batch 1000] loss: 0.968\n",
      "Validation loss: 0.986, Accuracy: 57.36%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Bach       0.72      0.93      0.81       788\n",
      "   Beethoven       0.45      0.17      0.25       445\n",
      "      Chopin       0.79      0.44      0.57       254\n",
      "      Mozart       0.32      0.52      0.39       236\n",
      "\n",
      "    accuracy                           0.61      1723\n",
      "   macro avg       0.57      0.52      0.50      1723\n",
      "weighted avg       0.60      0.61      0.57      1723\n",
      "\n",
      "[Epoch 1, Batch 1000] loss: 1.306\n",
      "Validation loss: 1.093, Accuracy: 45.59%\n",
      "[Epoch 2, Batch 1000] loss: 1.049\n",
      "Validation loss: 0.957, Accuracy: 58.21%\n",
      "[Epoch 3, Batch 1000] loss: 0.950\n",
      "Validation loss: 1.017, Accuracy: 56.40%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Bach       0.72      0.94      0.81       788\n",
      "   Beethoven       0.53      0.34      0.41       445\n",
      "      Chopin       0.89      0.22      0.35       254\n",
      "      Mozart       0.28      0.42      0.34       236\n",
      "\n",
      "    accuracy                           0.61      1723\n",
      "   macro avg       0.61      0.48      0.48      1723\n",
      "weighted avg       0.64      0.61      0.58      1723\n",
      "\n",
      "Validation loss: 1.074, Accuracy: 46.48%\n",
      "Validation loss: 1.146, Accuracy: 53.88%\n",
      "Validation loss: 1.020, Accuracy: 57.47%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Bach       0.75      0.87      0.81       366\n",
      "   Beethoven       0.57      0.43      0.49       216\n",
      "      Chopin       0.89      0.26      0.40       121\n",
      "      Mozart       0.28      0.47      0.35       112\n",
      "\n",
      "    accuracy                           0.61       815\n",
      "   macro avg       0.62      0.51      0.51       815\n",
      "weighted avg       0.66      0.61      0.60       815\n",
      "\n",
      "[Epoch 1, Batch 1000] loss: 1.082\n",
      "[Epoch 1, Batch 2000] loss: 0.970\n",
      "[Epoch 1, Batch 3000] loss: 0.918\n",
      "[Epoch 1, Batch 4000] loss: 0.897\n",
      "[Epoch 1, Batch 5000] loss: 0.866\n",
      "[Epoch 1, Batch 6000] loss: 0.849\n",
      "[Epoch 1, Batch 7000] loss: 0.833\n",
      "[Epoch 1, Batch 8000] loss: 0.846\n",
      "Validation loss: 0.854, Accuracy: 65.02%\n",
      "[Epoch 2, Batch 1000] loss: 0.808\n",
      "[Epoch 2, Batch 2000] loss: 0.822\n",
      "[Epoch 2, Batch 3000] loss: 0.793\n",
      "[Epoch 2, Batch 4000] loss: 0.810\n",
      "[Epoch 2, Batch 5000] loss: 0.801\n",
      "[Epoch 2, Batch 6000] loss: 0.802\n",
      "[Epoch 2, Batch 7000] loss: 0.783\n",
      "[Epoch 2, Batch 8000] loss: 0.789\n",
      "Validation loss: 0.841, Accuracy: 64.16%\n",
      "[Epoch 3, Batch 1000] loss: 0.760\n",
      "[Epoch 3, Batch 2000] loss: 0.765\n",
      "[Epoch 3, Batch 3000] loss: 0.773\n",
      "[Epoch 3, Batch 4000] loss: 0.749\n",
      "[Epoch 3, Batch 5000] loss: 0.759\n",
      "[Epoch 3, Batch 6000] loss: 0.741\n",
      "[Epoch 3, Batch 7000] loss: 0.752\n",
      "[Epoch 3, Batch 8000] loss: 0.763\n",
      "Validation loss: 0.843, Accuracy: 65.02%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Bach       0.86      0.87      0.86      3290\n",
      "   Beethoven       0.57      0.67      0.61      1814\n",
      "      Chopin       0.75      0.36      0.49      1047\n",
      "      Mozart       0.41      0.46      0.43       968\n",
      "\n",
      "    accuracy                           0.69      7119\n",
      "   macro avg       0.65      0.59      0.60      7119\n",
      "weighted avg       0.71      0.69      0.69      7119\n",
      "\n",
      "[Epoch 1, Batch 1000] loss: 1.046\n",
      "[Epoch 1, Batch 2000] loss: 0.888\n",
      "[Epoch 1, Batch 3000] loss: 0.831\n",
      "Validation loss: 0.855, Accuracy: 63.26%\n",
      "[Epoch 2, Batch 1000] loss: 0.788\n",
      "[Epoch 2, Batch 2000] loss: 0.745\n",
      "[Epoch 2, Batch 3000] loss: 0.751\n",
      "Validation loss: 0.818, Accuracy: 65.60%\n",
      "[Epoch 3, Batch 1000] loss: 0.676\n",
      "[Epoch 3, Batch 2000] loss: 0.723\n",
      "[Epoch 3, Batch 3000] loss: 0.717\n",
      "Validation loss: 0.826, Accuracy: 67.83%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Bach       0.87      0.86      0.87      1619\n",
      "   Beethoven       0.62      0.60      0.61       901\n",
      "      Chopin       0.85      0.31      0.46       518\n",
      "      Mozart       0.37      0.66      0.47       481\n",
      "\n",
      "    accuracy                           0.69      3519\n",
      "   macro avg       0.68      0.61      0.60      3519\n",
      "weighted avg       0.74      0.69      0.69      3519\n",
      "\n",
      "[Epoch 1, Batch 1000] loss: 0.970\n",
      "Validation loss: 0.970, Accuracy: 63.84%\n",
      "[Epoch 2, Batch 1000] loss: 0.699\n",
      "Validation loss: 0.722, Accuracy: 70.67%\n",
      "[Epoch 3, Batch 1000] loss: 0.591\n",
      "Validation loss: 0.809, Accuracy: 68.85%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Bach       0.89      0.92      0.91       788\n",
      "   Beethoven       0.62      0.73      0.67       445\n",
      "      Chopin       0.79      0.44      0.57       254\n",
      "      Mozart       0.48      0.49      0.48       236\n",
      "\n",
      "    accuracy                           0.74      1723\n",
      "   macro avg       0.69      0.65      0.66      1723\n",
      "weighted avg       0.75      0.74      0.74      1723\n",
      "\n",
      "[Epoch 1, Batch 1000] loss: 1.003\n",
      "Validation loss: 0.793, Accuracy: 67.20%\n",
      "[Epoch 2, Batch 1000] loss: 0.730\n",
      "Validation loss: 0.863, Accuracy: 65.26%\n",
      "[Epoch 3, Batch 1000] loss: 0.628\n",
      "Validation loss: 0.872, Accuracy: 66.17%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Bach       0.89      0.89      0.89       788\n",
      "   Beethoven       0.55      0.73      0.63       445\n",
      "      Chopin       0.86      0.17      0.28       254\n",
      "      Mozart       0.37      0.47      0.41       236\n",
      "\n",
      "    accuracy                           0.68      1723\n",
      "   macro avg       0.67      0.56      0.55      1723\n",
      "weighted avg       0.73      0.68      0.67      1723\n",
      "\n",
      "[Epoch 1, Batch 1000] loss: 1.017\n",
      "Validation loss: 0.880, Accuracy: 61.40%\n",
      "[Epoch 2, Batch 1000] loss: 0.751\n",
      "Validation loss: 0.806, Accuracy: 64.41%\n",
      "[Epoch 3, Batch 1000] loss: 0.645\n",
      "Validation loss: 0.835, Accuracy: 65.72%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Bach       0.90      0.89      0.90       788\n",
      "   Beethoven       0.55      0.80      0.65       445\n",
      "      Chopin       0.78      0.26      0.39       254\n",
      "      Mozart       0.48      0.42      0.45       236\n",
      "\n",
      "    accuracy                           0.71      1723\n",
      "   macro avg       0.68      0.59      0.59      1723\n",
      "weighted avg       0.73      0.71      0.70      1723\n",
      "\n",
      "Validation loss: 0.928, Accuracy: 61.53%\n",
      "Validation loss: 0.997, Accuracy: 65.23%\n",
      "Validation loss: 0.865, Accuracy: 67.62%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Bach       0.87      0.95      0.90       366\n",
      "   Beethoven       0.67      0.70      0.69       216\n",
      "      Chopin       0.86      0.35      0.49       121\n",
      "      Mozart       0.50      0.62      0.56       112\n",
      "\n",
      "    accuracy                           0.75       815\n",
      "   macro avg       0.72      0.66      0.66       815\n",
      "weighted avg       0.76      0.75      0.74       815\n",
      "\n",
      "Validation loss: 0.804, Accuracy: 65.35%\n",
      "Validation loss: 0.918, Accuracy: 63.32%\n",
      "Validation loss: 0.791, Accuracy: 69.77%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Bach       0.90      0.93      0.91       366\n",
      "   Beethoven       0.60      0.73      0.66       216\n",
      "      Chopin       0.89      0.28      0.43       121\n",
      "      Mozart       0.45      0.54      0.49       112\n",
      "\n",
      "    accuracy                           0.73       815\n",
      "   macro avg       0.71      0.62      0.62       815\n",
      "weighted avg       0.76      0.73      0.72       815\n",
      "\n",
      "Validation loss: 1.070, Accuracy: 54.00%\n",
      "Validation loss: 0.778, Accuracy: 68.70%\n",
      "Validation loss: 0.938, Accuracy: 69.65%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Bach       0.91      0.86      0.88       366\n",
      "   Beethoven       0.65      0.68      0.66       216\n",
      "      Chopin       0.89      0.42      0.57       121\n",
      "      Mozart       0.39      0.64      0.49       112\n",
      "\n",
      "    accuracy                           0.72       815\n",
      "   macro avg       0.71      0.65      0.65       815\n",
      "weighted avg       0.77      0.72      0.72       815\n",
      "\n",
      "Validation loss: 0.928, Accuracy: 61.53%\n",
      "Validation loss: 0.997, Accuracy: 65.23%\n",
      "Validation loss: 0.865, Accuracy: 67.62%\n",
      "Validation loss: 1.103, Accuracy: 68.34%\n",
      "Validation loss: 1.209, Accuracy: 67.98%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Bach       0.87      0.93      0.90       366\n",
      "   Beethoven       0.62      0.76      0.69       216\n",
      "      Chopin       0.79      0.36      0.50       121\n",
      "      Mozart       0.60      0.55      0.58       112\n",
      "\n",
      "    accuracy                           0.75       815\n",
      "   macro avg       0.72      0.65      0.67       815\n",
      "weighted avg       0.76      0.75      0.74       815\n",
      "\n",
      "Validation loss: 0.804, Accuracy: 65.35%\n",
      "Validation loss: 0.918, Accuracy: 63.32%\n",
      "Validation loss: 0.791, Accuracy: 69.77%\n",
      "Validation loss: 1.038, Accuracy: 68.34%\n",
      "Validation loss: 1.083, Accuracy: 68.46%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Bach       0.90      0.93      0.92       366\n",
      "   Beethoven       0.64      0.70      0.67       216\n",
      "      Chopin       0.72      0.61      0.66       121\n",
      "      Mozart       0.51      0.46      0.49       112\n",
      "\n",
      "    accuracy                           0.76       815\n",
      "   macro avg       0.70      0.68      0.68       815\n",
      "weighted avg       0.75      0.76      0.75       815\n",
      "\n",
      "Validation loss: 1.070, Accuracy: 54.00%\n",
      "Validation loss: 0.778, Accuracy: 68.70%\n",
      "Validation loss: 0.938, Accuracy: 69.65%\n",
      "Validation loss: 0.866, Accuracy: 71.57%\n",
      "Validation loss: 0.867, Accuracy: 73.36%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Bach       0.92      0.90      0.91       366\n",
      "   Beethoven       0.64      0.61      0.62       216\n",
      "      Chopin       0.86      0.45      0.59       121\n",
      "      Mozart       0.40      0.68      0.50       112\n",
      "\n",
      "    accuracy                           0.72       815\n",
      "   macro avg       0.70      0.66      0.66       815\n",
      "weighted avg       0.77      0.72      0.73       815\n",
      "\n",
      "[Epoch 1, Batch 1000] loss: 1.103\n",
      "[Epoch 1, Batch 2000] loss: 0.984\n",
      "[Epoch 1, Batch 3000] loss: 0.941\n",
      "[Epoch 1, Batch 4000] loss: 0.908\n",
      "[Epoch 1, Batch 5000] loss: 0.871\n",
      "[Epoch 1, Batch 6000] loss: 0.868\n",
      "[Epoch 1, Batch 7000] loss: 0.867\n",
      "[Epoch 1, Batch 8000] loss: 0.844\n",
      "Validation loss: 0.899, Accuracy: 63.57%\n",
      "[Epoch 2, Batch 1000] loss: 0.831\n",
      "[Epoch 2, Batch 2000] loss: 0.802\n",
      "[Epoch 2, Batch 3000] loss: 0.799\n",
      "[Epoch 2, Batch 4000] loss: 0.785\n",
      "[Epoch 2, Batch 5000] loss: 0.794\n",
      "[Epoch 2, Batch 6000] loss: 0.773\n",
      "[Epoch 2, Batch 7000] loss: 0.765\n",
      "[Epoch 2, Batch 8000] loss: 0.789\n",
      "Validation loss: 0.872, Accuracy: 65.02%\n",
      "[Epoch 3, Batch 1000] loss: 0.738\n",
      "[Epoch 3, Batch 2000] loss: 0.746\n",
      "[Epoch 3, Batch 3000] loss: 0.739\n",
      "[Epoch 3, Batch 4000] loss: 0.744\n",
      "[Epoch 3, Batch 5000] loss: 0.740\n",
      "[Epoch 3, Batch 6000] loss: 0.747\n",
      "[Epoch 3, Batch 7000] loss: 0.744\n",
      "[Epoch 3, Batch 8000] loss: 0.748\n",
      "Validation loss: 0.834, Accuracy: 66.79%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Bach       0.84      0.90      0.87      3290\n",
      "   Beethoven       0.58      0.66      0.62      1814\n",
      "      Chopin       0.79      0.29      0.42      1047\n",
      "      Mozart       0.40      0.48      0.43       968\n",
      "\n",
      "    accuracy                           0.69      7119\n",
      "   macro avg       0.65      0.58      0.59      7119\n",
      "weighted avg       0.71      0.69      0.68      7119\n",
      "\n",
      "[Epoch 1, Batch 1000] loss: 1.067\n",
      "[Epoch 1, Batch 2000] loss: 0.927\n",
      "[Epoch 1, Batch 3000] loss: 0.860\n",
      "Validation loss: 0.931, Accuracy: 63.79%\n",
      "[Epoch 2, Batch 1000] loss: 0.765\n",
      "[Epoch 2, Batch 2000] loss: 0.754\n",
      "[Epoch 2, Batch 3000] loss: 0.744\n",
      "Validation loss: 0.832, Accuracy: 65.77%\n",
      "[Epoch 3, Batch 1000] loss: 0.697\n",
      "[Epoch 3, Batch 2000] loss: 0.695\n",
      "[Epoch 3, Batch 3000] loss: 0.685\n",
      "Validation loss: 0.830, Accuracy: 67.21%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Bach       0.90      0.89      0.90      1619\n",
      "   Beethoven       0.60      0.76      0.67       901\n",
      "      Chopin       0.75      0.40      0.53       518\n",
      "      Mozart       0.47      0.47      0.47       481\n",
      "\n",
      "    accuracy                           0.73      3519\n",
      "   macro avg       0.68      0.63      0.64      3519\n",
      "weighted avg       0.74      0.73      0.73      3519\n",
      "\n",
      "[Epoch 1, Batch 1000] loss: 1.042\n",
      "Validation loss: 0.825, Accuracy: 65.04%\n",
      "[Epoch 2, Batch 1000] loss: 0.711\n",
      "Validation loss: 0.744, Accuracy: 69.41%\n",
      "[Epoch 3, Batch 1000] loss: 0.595\n",
      "Validation loss: 0.900, Accuracy: 68.33%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Bach       0.86      0.91      0.89       788\n",
      "   Beethoven       0.61      0.76      0.68       445\n",
      "      Chopin       0.88      0.28      0.42       254\n",
      "      Mozart       0.50      0.54      0.52       236\n",
      "\n",
      "    accuracy                           0.73      1723\n",
      "   macro avg       0.71      0.62      0.63      1723\n",
      "weighted avg       0.75      0.73      0.71      1723\n",
      "\n",
      "[Epoch 1, Batch 1000] loss: 1.038\n",
      "Validation loss: 0.918, Accuracy: 63.56%\n",
      "[Epoch 2, Batch 1000] loss: 0.777\n",
      "Validation loss: 0.906, Accuracy: 64.53%\n",
      "[Epoch 3, Batch 1000] loss: 0.655\n",
      "Validation loss: 0.788, Accuracy: 68.85%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Bach       0.90      0.84      0.87       788\n",
      "   Beethoven       0.67      0.61      0.64       445\n",
      "      Chopin       0.80      0.51      0.62       254\n",
      "      Mozart       0.38      0.68      0.49       236\n",
      "\n",
      "    accuracy                           0.71      1723\n",
      "   macro avg       0.69      0.66      0.65      1723\n",
      "weighted avg       0.75      0.71      0.72      1723\n",
      "\n",
      "[Epoch 1, Batch 1000] loss: 1.111\n",
      "Validation loss: 0.797, Accuracy: 65.83%\n",
      "[Epoch 2, Batch 1000] loss: 0.796\n",
      "Validation loss: 0.846, Accuracy: 65.72%\n",
      "[Epoch 3, Batch 1000] loss: 0.641\n",
      "Validation loss: 0.802, Accuracy: 68.28%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Bach       0.88      0.93      0.90       788\n",
      "   Beethoven       0.56      0.64      0.59       445\n",
      "      Chopin       0.92      0.19      0.32       254\n",
      "      Mozart       0.42      0.58      0.48       236\n",
      "\n",
      "    accuracy                           0.70      1723\n",
      "   macro avg       0.69      0.58      0.57      1723\n",
      "weighted avg       0.74      0.70      0.68      1723\n",
      "\n",
      "Validation loss: 0.820, Accuracy: 65.59%\n",
      "Validation loss: 0.771, Accuracy: 70.01%\n",
      "Validation loss: 0.896, Accuracy: 68.46%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Bach       0.88      0.94      0.91       366\n",
      "   Beethoven       0.56      0.63      0.59       216\n",
      "      Chopin       0.84      0.13      0.23       121\n",
      "      Mozart       0.37      0.53      0.44       112\n",
      "\n",
      "    accuracy                           0.68       815\n",
      "   macro avg       0.66      0.56      0.54       815\n",
      "weighted avg       0.72      0.68      0.66       815\n",
      "\n",
      "Validation loss: 1.103, Accuracy: 53.64%\n",
      "Validation loss: 0.917, Accuracy: 61.41%\n",
      "Validation loss: 1.296, Accuracy: 46.24%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Bach       0.87      0.70      0.77       366\n",
      "   Beethoven       0.47      0.77      0.58       216\n",
      "      Chopin       0.54      0.34      0.42       121\n",
      "      Mozart       0.37      0.29      0.32       112\n",
      "\n",
      "    accuracy                           0.61       815\n",
      "   macro avg       0.56      0.52      0.52       815\n",
      "weighted avg       0.64      0.61      0.61       815\n",
      "\n",
      "Validation loss: 0.902, Accuracy: 61.65%\n",
      "Validation loss: 0.980, Accuracy: 60.45%\n",
      "Validation loss: 1.077, Accuracy: 64.28%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Bach       0.92      0.87      0.90       366\n",
      "   Beethoven       0.52      0.84      0.64       216\n",
      "      Chopin       0.91      0.08      0.15       121\n",
      "      Mozart       0.48      0.47      0.48       112\n",
      "\n",
      "    accuracy                           0.69       815\n",
      "   macro avg       0.71      0.57      0.54       815\n",
      "weighted avg       0.75      0.69      0.66       815\n",
      "\n",
      "Validation loss: 0.820, Accuracy: 65.59%\n",
      "Validation loss: 0.771, Accuracy: 70.01%\n",
      "Validation loss: 0.896, Accuracy: 68.46%\n",
      "Validation loss: 1.089, Accuracy: 68.34%\n",
      "Validation loss: 1.161, Accuracy: 69.30%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Bach       0.86      0.92      0.89       366\n",
      "   Beethoven       0.64      0.69      0.67       216\n",
      "      Chopin       0.75      0.36      0.48       121\n",
      "      Mozart       0.41      0.49      0.45       112\n",
      "\n",
      "    accuracy                           0.71       815\n",
      "   macro avg       0.67      0.61      0.62       815\n",
      "weighted avg       0.72      0.71      0.71       815\n",
      "\n",
      "Validation loss: 1.103, Accuracy: 53.64%\n",
      "Validation loss: 0.917, Accuracy: 61.41%\n",
      "Validation loss: 1.296, Accuracy: 46.24%\n",
      "Validation loss: 1.026, Accuracy: 62.37%\n",
      "Validation loss: 1.428, Accuracy: 63.32%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Bach       0.79      0.96      0.87       366\n",
      "   Beethoven       0.64      0.69      0.67       216\n",
      "      Chopin       0.77      0.34      0.47       121\n",
      "      Mozart       0.52      0.39      0.45       112\n",
      "\n",
      "    accuracy                           0.72       815\n",
      "   macro avg       0.68      0.60      0.61       815\n",
      "weighted avg       0.71      0.72      0.70       815\n",
      "\n",
      "Validation loss: 0.902, Accuracy: 61.65%\n",
      "Validation loss: 0.980, Accuracy: 60.45%\n",
      "Validation loss: 1.077, Accuracy: 64.28%\n",
      "Validation loss: 0.761, Accuracy: 70.73%\n",
      "Validation loss: 1.023, Accuracy: 67.74%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Bach       0.83      0.98      0.90       366\n",
      "   Beethoven       0.67      0.78      0.72       216\n",
      "      Chopin       0.86      0.31      0.46       121\n",
      "      Mozart       0.60      0.47      0.53       112\n",
      "\n",
      "    accuracy                           0.76       815\n",
      "   macro avg       0.74      0.64      0.65       815\n",
      "weighted avg       0.76      0.76      0.73       815\n",
      "\n",
      "[Epoch 1, Batch 1000] loss: 1.017\n",
      "Validation loss: 0.880, Accuracy: 61.40%\n",
      "[Epoch 2, Batch 1000] loss: 0.751\n",
      "Validation loss: 0.806, Accuracy: 64.41%\n",
      "[Epoch 3, Batch 1000] loss: 0.645\n",
      "Validation loss: 0.835, Accuracy: 65.72%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Bach       0.90      0.89      0.90       788\n",
      "   Beethoven       0.55      0.80      0.65       445\n",
      "      Chopin       0.78      0.26      0.39       254\n",
      "      Mozart       0.48      0.42      0.45       236\n",
      "\n",
      "    accuracy                           0.71      1723\n",
      "   macro avg       0.68      0.59      0.59      1723\n",
      "weighted avg       0.73      0.71      0.70      1723\n",
      "\n",
      "[Epoch 1, Batch 1000] loss: 1.017\n",
      "Validation loss: 0.880, Accuracy: 61.40%\n",
      "[Epoch 2, Batch 1000] loss: 0.751\n",
      "Validation loss: 0.806, Accuracy: 64.41%\n",
      "[Epoch 3, Batch 1000] loss: 0.645\n",
      "Validation loss: 0.835, Accuracy: 65.72%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Bach       0.90      0.89      0.90       788\n",
      "   Beethoven       0.55      0.80      0.65       445\n",
      "      Chopin       0.78      0.26      0.39       254\n",
      "      Mozart       0.48      0.42      0.45       236\n",
      "\n",
      "    accuracy                           0.71      1723\n",
      "   macro avg       0.68      0.59      0.59      1723\n",
      "weighted avg       0.73      0.71      0.70      1723\n",
      "\n",
      "[Epoch 1, Batch 1000] loss: 1.017\n",
      "Validation loss: 0.880, Accuracy: 61.40%\n",
      "[Epoch 2, Batch 1000] loss: 0.751\n",
      "Validation loss: 0.806, Accuracy: 64.41%\n",
      "[Epoch 3, Batch 1000] loss: 0.645\n",
      "Validation loss: 0.835, Accuracy: 65.72%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Bach       0.90      0.89      0.90       788\n",
      "   Beethoven       0.55      0.80      0.65       445\n",
      "      Chopin       0.78      0.26      0.39       254\n",
      "      Mozart       0.48      0.42      0.45       236\n",
      "\n",
      "    accuracy                           0.71      1723\n",
      "   macro avg       0.68      0.59      0.59      1723\n",
      "weighted avg       0.73      0.71      0.70      1723\n",
      "\n",
      "[Epoch 1, Batch 1000] loss: 1.017\n",
      "Validation loss: 0.880, Accuracy: 61.40%\n",
      "[Epoch 2, Batch 1000] loss: 0.751\n",
      "Validation loss: 0.806, Accuracy: 64.41%\n",
      "[Epoch 3, Batch 1000] loss: 0.645\n",
      "Validation loss: 0.835, Accuracy: 65.72%\n",
      "[Epoch 4, Batch 1000] loss: 0.557\n",
      "Validation loss: 0.827, Accuracy: 68.45%\n",
      "[Epoch 5, Batch 1000] loss: 0.490\n",
      "Validation loss: 0.967, Accuracy: 68.33%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Bach       0.88      0.93      0.90       788\n",
      "   Beethoven       0.60      0.74      0.66       445\n",
      "      Chopin       0.71      0.37      0.49       254\n",
      "      Mozart       0.55      0.49      0.52       236\n",
      "\n",
      "    accuracy                           0.74      1723\n",
      "   macro avg       0.68      0.63      0.64      1723\n",
      "weighted avg       0.74      0.74      0.73      1723\n",
      "\n",
      "[Epoch 1, Batch 1000] loss: 1.017\n",
      "Validation loss: 0.880, Accuracy: 61.40%\n",
      "[Epoch 2, Batch 1000] loss: 0.751\n",
      "Validation loss: 0.806, Accuracy: 64.41%\n",
      "[Epoch 3, Batch 1000] loss: 0.645\n",
      "Validation loss: 0.835, Accuracy: 65.72%\n",
      "[Epoch 4, Batch 1000] loss: 0.557\n",
      "Validation loss: 0.827, Accuracy: 68.45%\n",
      "[Epoch 5, Batch 1000] loss: 0.490\n",
      "Validation loss: 0.967, Accuracy: 68.33%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Bach       0.88      0.93      0.90       788\n",
      "   Beethoven       0.60      0.74      0.66       445\n",
      "      Chopin       0.71      0.37      0.49       254\n",
      "      Mozart       0.55      0.49      0.52       236\n",
      "\n",
      "    accuracy                           0.74      1723\n",
      "   macro avg       0.68      0.63      0.64      1723\n",
      "weighted avg       0.74      0.74      0.73      1723\n",
      "\n",
      "[Epoch 1, Batch 1000] loss: 1.017\n",
      "Validation loss: 0.880, Accuracy: 61.40%\n",
      "[Epoch 2, Batch 1000] loss: 0.751\n",
      "Validation loss: 0.806, Accuracy: 64.41%\n",
      "[Epoch 3, Batch 1000] loss: 0.645\n",
      "Validation loss: 0.835, Accuracy: 65.72%\n",
      "[Epoch 4, Batch 1000] loss: 0.557\n",
      "Validation loss: 0.827, Accuracy: 68.45%\n",
      "[Epoch 5, Batch 1000] loss: 0.490\n",
      "Validation loss: 0.967, Accuracy: 68.33%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Bach       0.88      0.93      0.90       788\n",
      "   Beethoven       0.60      0.74      0.66       445\n",
      "      Chopin       0.71      0.37      0.49       254\n",
      "      Mozart       0.55      0.49      0.52       236\n",
      "\n",
      "    accuracy                           0.74      1723\n",
      "   macro avg       0.68      0.63      0.64      1723\n",
      "weighted avg       0.74      0.74      0.73      1723\n",
      "\n",
      "[Epoch 1, Batch 1000] loss: 1.017\n",
      "Validation loss: 0.880, Accuracy: 61.40%\n",
      "[Epoch 2, Batch 1000] loss: 0.751\n",
      "Validation loss: 0.806, Accuracy: 64.41%\n",
      "[Epoch 3, Batch 1000] loss: 0.645\n",
      "Validation loss: 0.835, Accuracy: 65.72%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Bach       0.90      0.89      0.90       788\n",
      "   Beethoven       0.55      0.80      0.65       445\n",
      "      Chopin       0.78      0.26      0.39       254\n",
      "      Mozart       0.48      0.42      0.45       236\n",
      "\n",
      "    accuracy                           0.71      1723\n",
      "   macro avg       0.68      0.59      0.59      1723\n",
      "weighted avg       0.73      0.71      0.70      1723\n",
      "\n",
      "[Epoch 1, Batch 1000] loss: 1.017\n",
      "Validation loss: 0.880, Accuracy: 61.40%\n",
      "[Epoch 2, Batch 1000] loss: 0.751\n",
      "Validation loss: 0.806, Accuracy: 64.41%\n",
      "[Epoch 3, Batch 1000] loss: 0.645\n",
      "Validation loss: 0.835, Accuracy: 65.72%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Bach       0.90      0.89      0.90       788\n",
      "   Beethoven       0.55      0.80      0.65       445\n",
      "      Chopin       0.78      0.26      0.39       254\n",
      "      Mozart       0.48      0.42      0.45       236\n",
      "\n",
      "    accuracy                           0.71      1723\n",
      "   macro avg       0.68      0.59      0.59      1723\n",
      "weighted avg       0.73      0.71      0.70      1723\n",
      "\n",
      "[Epoch 1, Batch 1000] loss: 1.017\n",
      "Validation loss: 0.880, Accuracy: 61.40%\n",
      "[Epoch 2, Batch 1000] loss: 0.751\n",
      "Validation loss: 0.806, Accuracy: 64.41%\n",
      "[Epoch 3, Batch 1000] loss: 0.645\n",
      "Validation loss: 0.835, Accuracy: 65.72%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Bach       0.90      0.89      0.90       788\n",
      "   Beethoven       0.55      0.80      0.65       445\n",
      "      Chopin       0.78      0.26      0.39       254\n",
      "      Mozart       0.48      0.42      0.45       236\n",
      "\n",
      "    accuracy                           0.71      1723\n",
      "   macro avg       0.68      0.59      0.59      1723\n",
      "weighted avg       0.73      0.71      0.70      1723\n",
      "\n"
     ]
    }
   ],
   "source": [
    "process_file('cnn_optimization.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b35cd3d",
   "metadata": {},
   "source": [
    "## Single Training Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "415499ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'dropout'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 18\u001b[0m\n\u001b[1;32m     14\u001b[0m test_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpool_stride\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     16\u001b[0m test_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfc1_out\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m\n\u001b[0;32m---> 18\u001b[0m model, test_dataloader \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m report, _, _ \u001b[38;5;241m=\u001b[39m evaluate_performance(model, test_dataloader)\n",
      "Cell \u001b[0;32mIn[12], line 8\u001b[0m, in \u001b[0;36mtraining_run\u001b[0;34m(h_params)\u001b[0m\n\u001b[1;32m      4\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(h_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      6\u001b[0m train_loader, val_loader, test_loader \u001b[38;5;241m=\u001b[39m get_dataloaders(h_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_chroma\u001b[39m\u001b[38;5;124m'\u001b[39m], seq_length, batch_size)\n\u001b[0;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mPianoRollCNN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m     11\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mh_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[9], line 31\u001b[0m, in \u001b[0;36mPianoRollCNN.__init__\u001b[0;34m(self, h_params)\u001b[0m\n\u001b[1;32m     28\u001b[0m pooled_width \u001b[38;5;241m=\u001b[39m conv_output_size(conv2_output_width, h_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpool_kernel_size\u001b[39m\u001b[38;5;124m'\u001b[39m], h_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpool_stride\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(h_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconv2_out_channels\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m pooled_width, h_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfc1_out\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDropout(\u001b[43mh_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdropout\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(h_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfc1_out\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;241m4\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'dropout'"
     ]
    }
   ],
   "source": [
    "test_params = {}\n",
    "test_params['use_chroma'] = True\n",
    "test_params['seq_length'] = 50\n",
    "test_params['batch_size'] = 8\n",
    "test_params['lr'] = 0.001\n",
    "test_params['num_epochs'] = 1\n",
    "\n",
    "test_params['conv1_out_channels'] = 32\n",
    "test_params['conv2_out_channels'] = 64\n",
    "test_params['kernel_size'] = 3\n",
    "test_params['stride'] = 1\n",
    "\n",
    "test_params['pool_kernel_size'] = 2\n",
    "test_params['pool_stride'] = 2\n",
    "\n",
    "test_params['fc1_out'] = 128\n",
    "\n",
    "model, test_dataloader = training_run(test_params)\n",
    "\n",
    "report, _, _ = evaluate_performance(model, test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
